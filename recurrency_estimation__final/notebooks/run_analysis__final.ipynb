{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb34708-4e0f-4765-91a3-11bddcf0f96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/loidolt/RowlandEtAl/popping-off/popoff/popoff/loadpaths.py\n",
      "/home/loidolt/RowlandEtAl/Vape\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import recurrency_estimation as re\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy.stats\n",
    "from dask_jobqueue import SGECluster\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import xarray as xr\n",
    "import itertools\n",
    "import time\n",
    "import matplotlib\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.special import comb\n",
    "\n",
    "from Session import SessionLite # required to unpickle data\n",
    "from linear_model import PoolAcrossSessions, LinearModel, MultiSessionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "078e4f65-b5c9-4c70-ba80-cc1532c3ab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "378112a5-42a8-48f0-afeb-636d7986d6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_tt = {'hit': '#117733', 'miss': '#882255', 'fp': '#88CCEE', 'cr': '#DDCC77',\n",
    "            'Hit': '#117733', 'Miss': '#882255', 'FP': '#88CCEE', 'CR': '#DDCC77',\n",
    "            'urh': '#44AA99', 'arm': '#AA4499', 'spont': '#332288', 'prereward': '#332288', \n",
    "            'reward\\nonly': '#332288', 'Reward\\nonly': '#332288',\n",
    "            'pre_reward': '#332288', 'Reward': '#332288', 'reward only': '#332288', 'rew. only': '#332288', 'hit&miss': 'k', \n",
    "            'fp&cr': 'k', 'photostim': sns.color_palette()[6], 'too_': 'grey',\n",
    "            'hit_n1': '#b0eac9', 'hit_n2': '#5ab17f', 'hit_n3': '#117733',\n",
    "            'miss_n1': '#a69098', 'miss_n2': '#985d76', 'miss_n3': '#882255',\n",
    "            'hit_c1': '#b0eac9', 'hit_c2': '#5ab17f', 'hit_c3': '#117733',\n",
    "            'miss_c1': '#a69098', 'miss_c2': '#985d76', 'miss_c3': '#882255'\n",
    "            } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a571c992-6154-4f82-a0d5-5a2618168bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams[\"xtick.labelsize\"] = 10\n",
    "matplotlib.rcParams[\"ytick.labelsize\"] = 10\n",
    "matplotlib.rcParams[\"axes.labelsize\"] = 10\n",
    "matplotlib.rcParams[\"axes.titlesize\"]= 10\n",
    "matplotlib.rcParams[\"legend.fontsize\"] = 10\n",
    "matplotlib.rcParams[\"legend.title_fontsize\"] = 10\n",
    "\n",
    "matplotlib.rcParams[\"axes.spines.right\"] = True\n",
    "matplotlib.rcParams[\"axes.spines.top\"] = True\n",
    "matplotlib.rcParams[\"axes.spines.left\"] = True\n",
    "matplotlib.rcParams[\"axes.spines.bottom\"] = True\n",
    "\n",
    "matplotlib.rcParams['axes.linewidth'] = 0.75\n",
    "matplotlib.rcParams[\"axes.labelcolor\"] = \"black\"\n",
    "matplotlib.rcParams[\"axes.edgecolor\"] = \"black\"\n",
    "matplotlib.rcParams[\"xtick.color\"] = \"black\"\n",
    "matplotlib.rcParams[\"ytick.color\"] = \"black\"\n",
    "\n",
    "matplotlib.rcParams['xtick.major.size'] = 3\n",
    "matplotlib.rcParams['xtick.major.width'] = 0.75\n",
    "matplotlib.rcParams['xtick.minor.size'] = 3\n",
    "matplotlib.rcParams['xtick.minor.width'] = 0.75\n",
    "\n",
    "matplotlib.rcParams['ytick.major.size'] = 3\n",
    "matplotlib.rcParams['ytick.major.width'] = 0.75\n",
    "matplotlib.rcParams['ytick.minor.size'] = 3\n",
    "matplotlib.rcParams['ytick.minor.width'] = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a64bbbaf-e173-4499-a98f-6406074a8b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long post time\n",
      "long post time\n",
      "long post time\n",
      "long post time\n",
      "long post time\n",
      "Mouse RL070, run 29  registered no-lick hit. changed to too soon\n",
      "long post time\n",
      "long post time\n",
      "Mouse RL117, run 29  registered no-lick hit. changed to too soon\n",
      "Mouse RL117, run 29  registered no-lick hit. changed to too soon\n",
      "long post time\n",
      "long post time\n",
      "long post time\n",
      "Mouse RL116, run 32  registered no-lick hit. changed to too soon\n",
      "Mouse RL116, run 32  registered no-lick hit. changed to too soon\n",
      "Mouse RL116, run 32  registered no-lick hit. changed to too soon\n",
      "long post time\n",
      "ALERT SESSIONS NOT SUBSAMPLED\n",
      "long post time\n",
      "long post time\n",
      "long post time\n",
      "long post time\n",
      "long post time\n",
      "long post time\n",
      "long post time\n",
      "long post time\n",
      "long post time\n",
      "long post time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1/11 [00:00<00:01,  7.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long post time\n",
      "{0: instance Mouse J064, run 10 of Session class, 1: instance Mouse J064, run 11 of Session class, 2: instance Mouse J064, run 14 of Session class, 3: instance Mouse RL070, run 28 of Session class, 4: instance Mouse RL070, run 29 of Session class, 5: instance Mouse RL117, run 26 of Session class, 6: instance Mouse RL117, run 29 of Session class, 7: instance Mouse RL117, run 30 of Session class, 8: instance Mouse RL123, run 22 of Session class, 9: instance Mouse RL116, run 32 of Session class, 10: instance Mouse RL116, run 33 of Session class}\n",
      "URH and ARM trials have been labelled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:02<00:00,  4.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# load data for for latent factor and recurrence analysis\n",
    "session_dict = re.load_data(remove_toosoon=True, label_urh_arm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cc3203",
   "metadata": {},
   "outputs": [],
   "source": [
    "#per var explained -- LFA\n",
    "n_sessions = len(session_dict.keys())\n",
    "num_fits = 1000\n",
    "num_factors = 5\n",
    "num_frames = 65\n",
    "\n",
    "hit_per_var_explained = np.zeros((n_sessions, num_fits, num_factors))\n",
    "hit_per_var_explained[:] = np.nan\n",
    "\n",
    "hit_cosine_similarity = np.zeros((n_sessions, num_fits, num_factors))\n",
    "hit_cosine_similarity[:] = np.nan\n",
    "\n",
    "hit_peristim_average = np.zeros((n_sessions, num_frames, num_factors))\n",
    "hit_peristim_average[:] = np.nan\n",
    "\n",
    "miss_per_var_explained = np.zeros((n_sessions, num_fits, num_factors))\n",
    "miss_per_var_explained[:] = np.nan\n",
    "\n",
    "miss_cosine_similarity = np.zeros((n_sessions, num_fits, num_factors))\n",
    "miss_cosine_similarity[:] = np.nan\n",
    "\n",
    "miss_peristim_average = np.zeros((n_sessions, num_frames, num_factors))\n",
    "miss_peristim_average[:] = np.nan\n",
    "\n",
    "i_session = -1\n",
    "\n",
    "for session_name, session in session_dict.items():\n",
    "    i_session += 1\n",
    "    res = re.factor_analysis.per_var_explained_of_session(session, num_factors, num_fits, fa_type='lfa')\n",
    "    \n",
    "    hit_per_var_explained[i_session] = res[0]  \n",
    "    miss_per_var_explained[i_session] = res[3]\n",
    "\n",
    "np.save('../final_data/LFA.T_per_var_explained_hit_1000fits_5factors.npy', hit_per_var_explained)\n",
    "np.save('../final_data/LFA.T_per_var_explained_miss_1000fits_5factors.npy', miss_per_var_explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4fc1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "18fb4633-1e80-4633-99b9-12beda63f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Connect to cluster for Recurrence analysis\n",
    "try:\n",
    "    client = await Client('tcp://172.9.50.181:46466', asynchronous=True)\n",
    "except OSError:\n",
    "    cluster = await LocalCluster()\n",
    "    client = await Client(cluster, asynchronous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "58cd7952-4095-4d78-a496-f1ebfcde694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recurrence analysis\n",
    "num_fits = 1000 # Should be 1000, but 40 gives already good results \n",
    "n_factors_list = [21,22,23,24,25] # select factors to run here\n",
    "num_bootstrap = 0\n",
    "activity_type_list = ['shared', 'residual']\n",
    "parameter_names = ('n_fact', 'session','activity_type')\n",
    "parameter_values = (n_factors_list, list(session_dict.keys()), activity_type_list)\n",
    "\n",
    "time_begin = time.time()\n",
    "\n",
    "### Define LFA\n",
    "session_dict_delayed={}\n",
    "for session_name,session in session_dict.items():\n",
    "    #### select S1 or S2 here\n",
    "    session_dict_delayed[session_name] = dask.delayed(session.sel(area=1))\n",
    "\n",
    "    \n",
    "lfa_func = dask.delayed(re.factor_analysis.split_shared_residual_part, nout=2)\n",
    "nested_session_dict = {}\n",
    "for n_fact in n_factors_list:\n",
    "    nested_session_dict[n_fact] = {key: {} for key in activity_type_list}\n",
    "    for session_name,session in session_dict_delayed.items():\n",
    "        if n_fact == 0:\n",
    "            nested_session_dict[n_fact]['shared'][session_name] = session\n",
    "            nested_session_dict[n_fact]['residual'][session_name] = session\n",
    "            continue\n",
    "        #### select LFA or PCA here\n",
    "        session_shared, session_residual = lfa_func(session, n_fact=n_fact, fa_type='lfa')\n",
    "        nested_session_dict[n_fact]['shared'][session_name] = session_shared\n",
    "        nested_session_dict[n_fact]['residual'][session_name] = session_residual\n",
    "\n",
    "### Define recurrency calculation        \n",
    "delayed_dict = {}\n",
    "\n",
    "recurrency_func = dask.delayed(re.variance_cc.var_cc_of_session)\n",
    "for params_to_calc in itertools.product(*parameter_values):\n",
    "    n_fact, session, activity_type = params_to_calc\n",
    "    delayed_task = recurrency_func(nested_session_dict[n_fact][activity_type][session], num_bootstrap, num_fits=num_fits)\n",
    "    delayed_dict[params_to_calc] = delayed_task\n",
    "\n",
    "    \n",
    "### Begin calculatation\n",
    "future_dict = client.compute(delayed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb39bdd7-24c8-4f1e-9276-f14728361b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save results in xarray and to disc\n",
    "\n",
    "result_dict = await future_dict.result()\n",
    "\n",
    "res_dataset = xr.DataArray(data=None,  coords = [*zip(parameter_names, parameter_values)]+\n",
    "                                                 [('variable',['var_cc', 'mean_var','mean_rho']),\n",
    "                                                 ('trial_type',['hit', 'miss']),\n",
    "                                                 ('sample',np.arange(num_bootstrap+1))])\n",
    "for params_res, res in result_dict.items():\n",
    "    n_fact, session, activity_type = params_res\n",
    "    var_cc_hit, var_cc_miss, mean_var_hit, mean_var_miss, mean_rho_hit, mean_rho_miss = res\n",
    "    res_dataset.loc[dict([*zip(parameter_names, params_res)])] = np.array([[var_cc_hit, var_cc_miss], [mean_var_hit, mean_var_miss], [mean_rho_hit, mean_rho_miss]])\n",
    "\n",
    "res_dataset.to_netcdf(f\"../final_data/ML_nfac21-25_results_{num_fits}fits.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a382df32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68185b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data for tau_post and recurrence analysis\n",
    "session_dict = re.load_data_prepost(remove_toosoon=True, label_urh_arm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd12d388-5999-40d0-a48c-1a25549d88f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'session_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m activity_type_list \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mshared\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mresidual\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m parameter_names \u001b[39m=\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mn_fact\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msession\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mactivity_type\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m parameter_values \u001b[39m=\u001b[39m (n_factors_list, \u001b[39mlist\u001b[39m(session_dict\u001b[39m.\u001b[39mkeys()), activity_type_list)\n\u001b[1;32m      9\u001b[0m time_begin \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     11\u001b[0m \u001b[39m### Define LFA\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'session_dict' is not defined"
     ]
    }
   ],
   "source": [
    "## HitAndMiss\n",
    "num_fits = 1000 # Should be 1000, but 40 gives already good results \n",
    "n_factors_list = [0,5]\n",
    "num_bootstrap = 0\n",
    "activity_type_list = ['shared', 'residual']\n",
    "parameter_names = ('n_fact', 'session', 'activity_type')\n",
    "parameter_values = (n_factors_list, list(session_dict.keys()), activity_type_list)\n",
    "\n",
    "time_begin = time.time()\n",
    "\n",
    "### Define LFA\n",
    "session_dict_delayed={}\n",
    "for session_name,session in session_dict.items():\n",
    "    session_dict_delayed[session_name] = dask.delayed(session.sel(area=1))\n",
    "\n",
    "    \n",
    "lfa_func = dask.delayed(re.factor_analysis.split_shared_residual_part, nout=2)\n",
    "nested_session_dict = {}\n",
    "for n_fact in n_factors_list:\n",
    "    nested_session_dict[n_fact] = {key: {} for key in activity_type_list}\n",
    "    for session_name,session in session_dict_delayed.items():\n",
    "        if n_fact == 0:\n",
    "            nested_session_dict[n_fact]['shared'][session_name] = session\n",
    "            nested_session_dict[n_fact]['residual'][session_name] = session\n",
    "            continue\n",
    "        session_shared, session_residual = lfa_func(session, n_fact=n_fact, fa_type='lfa')\n",
    "        nested_session_dict[n_fact]['shared'][session_name] = session_shared\n",
    "        nested_session_dict[n_fact]['residual'][session_name] = session_residual\n",
    "\n",
    "### Define recurrency calculation        \n",
    "delayed_dict = {}\n",
    "recurrency_func = dask.delayed(re.prepost_var_cc_and_taupost_of_session_TestOnly_HitAndMiss)\n",
    "for params_to_calc in itertools.product(*parameter_values):\n",
    "    n_fact, session, activity_type = params_to_calc\n",
    "    delayed_task = recurrency_func(nested_session_dict[n_fact][activity_type][session], num_bootstrap, num_fits=num_fits)\n",
    "    delayed_dict[params_to_calc] = delayed_task\n",
    "\n",
    "    \n",
    "### Begin calculatation\n",
    "future_dict = client.compute(delayed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c890a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save results in xarray and to disc\n",
    "\n",
    "result_dict = await future_dict.result()\n",
    "\n",
    "res_dataset = xr.DataArray(data=None,  coords = [*zip(parameter_names, parameter_values)]+\n",
    "                                                 [('variable',['var_cc', 'mean_var','mean_rho', 'taupost']),\n",
    "                                                 ('trial_type',['HitAndMiss']),\n",
    "                                                 ('realization',np.arange(num_fits))])\n",
    "for params_res, res in result_dict.items():\n",
    "    n_fact, session, activity_type = params_res\n",
    "    var_cc_hit, mean_var_hit, mean_rho_hit, taupost_hit = res\n",
    "    res_dataset.loc[dict([*zip(parameter_names, params_res)])] = np.array([var_cc_hit, mean_var_hit, mean_rho_hit, taupost_hit])\n",
    "\n",
    "res_dataset.to_netcdf(f\"../data/ML_LFA_nfac0,5_S1_TestHitAndMiss_taupost_results_{num_fits}fits.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2647e09d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085cd40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tau_post (pulse-based recurrence) analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efa2a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "msm = MultiSessionModel(remove_targets=False, subsample_sessions=False,\n",
    "                         remove_toosoon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cc16d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_func(t, A, tau):\n",
    "    return A*np.exp(-t/tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694a184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_perms = 1000\n",
    "\n",
    "hit_tau_hats = np.zeros((n_sessions, max_perms))\n",
    "hit_tau_hats[:] = np.nan\n",
    "miss_tau_hats = np.zeros((n_sessions, max_perms))\n",
    "miss_tau_hats[:] = np.nan\n",
    "\n",
    "for i_session in range(n_sessions):\n",
    "    session = msm.sessions[i_session]\n",
    "    \n",
    "    targeted_neurons = np.any(session.is_target[:,:,0], axis=1)\n",
    "    \n",
    "    n_neurons = session.behaviour_trials.shape[0]\n",
    "    n_frames = session.behaviour_trials.shape[2]\n",
    "    \n",
    "    noStim_trials = np.nonzero(session.photostim==0)[0]\n",
    "    testStim_trials = np.nonzero(session.photostim==1)[0]\n",
    "    easyStim_trials = np.nonzero(session.photostim==2)[0]\n",
    "    \n",
    "    hitTrials = np.nonzero(session.outcome=='hit')[0]\n",
    "    missTrials = np.nonzero(session.outcome=='miss')[0]\n",
    "    \n",
    "    #testHitTrials = hitTrials\n",
    "    #testMissTrials = missTrials\n",
    "    \n",
    "    testHitTrials = np.intersect1d(testStim_trials, hitTrials)\n",
    "    testMissTrials = np.intersect1d(testStim_trials, missTrials)\n",
    "    \n",
    "    num_perms = int(comb(min([len(testHitTrials), len(testHitTrials)]), 10))\n",
    "    \n",
    "    if num_perms > max_perms:\n",
    "        num_perms = max_perms\n",
    "    \n",
    "    for i_perm in range(num_perms):\n",
    "        subHitTrials = np.random.choice(testHitTrials, 10, replace=False)\n",
    "        subMissTrials = np.random.choice(testMissTrials, 10, replace=False)\n",
    "    \n",
    "        testHit_TargetNeuronAverages = np.zeros((n_neurons,n_frames))\n",
    "        testHit_TargetNeuronAverages[:] = np.nan\n",
    "        noHit_TargetNeuronAverages = np.zeros((n_neurons,n_frames))\n",
    "        noHit_TargetNeuronAverages[:] = np.nan\n",
    "        testMiss_TargetNeuronAverages = np.zeros((n_neurons,n_frames))\n",
    "        testMiss_TargetNeuronAverages[:] = np.nan\n",
    "        noMiss_TargetNeuronAverages = np.zeros((n_neurons,n_frames))\n",
    "        noMiss_TargetNeuronAverages[:] = np.nan\n",
    "\n",
    "        testHit_NonTargetNeuronAverages = np.zeros((n_neurons,n_frames))\n",
    "        testHit_NonTargetNeuronAverages[:] = np.nan\n",
    "        noHit_NonTargetNeuronAverages = np.zeros((n_neurons,n_frames))\n",
    "        noHit_NonTargetNeuronAverages[:] = np.nan\n",
    "        testMiss_NonTargetNeuronAverages = np.zeros((n_neurons,n_frames))\n",
    "        testMiss_NonTargetNeuronAverages[:] = np.nan\n",
    "        noMiss_NonTargetNeuronAverages = np.zeros((n_neurons,n_frames))\n",
    "        noMiss_NonTargetNeuronAverages[:] = np.nan\n",
    "\n",
    "        for i_neuron in np.arange(n_neurons)[targeted_neurons]:\n",
    "            testHit_TargetNeuronAverages[i_neuron] = np.nanmean(session.behaviour_trials[i_neuron][subHitTrials[session.is_target[i_neuron,subHitTrials,0]]], axis=0)\n",
    "            noHit_TargetNeuronAverages[i_neuron] = np.nanmean(session.behaviour_trials[i_neuron][subHitTrials[~session.is_target[i_neuron,subHitTrials,0]]], axis=0)\n",
    "            testMiss_TargetNeuronAverages[i_neuron] = np.nanmean(session.behaviour_trials[i_neuron][subMissTrials[session.is_target[i_neuron,subMissTrials,0]]], axis=0)\n",
    "            noMiss_TargetNeuronAverages[i_neuron] = np.nanmean(session.behaviour_trials[i_neuron][subMissTrials[~session.is_target[i_neuron,subMissTrials,0]]], axis=0)\n",
    "\n",
    "        for i_neuron in np.arange(n_neurons)[~targeted_neurons]:\n",
    "            testHit_NonTargetNeuronAverages[i_neuron] = np.nanmean(session.behaviour_trials[i_neuron][subHitTrials], axis=0)\n",
    "            noHit_NonTargetNeuronAverages[i_neuron] = np.nanmean(session.behaviour_trials[i_neuron][subHitTrials], axis=0)\n",
    "            testMiss_NonTargetNeuronAverages[i_neuron] = np.nanmean(session.behaviour_trials[i_neuron][subMissTrials], axis=0)\n",
    "            noMiss_NonTargetNeuronAverages[i_neuron] = np.nanmean(session.behaviour_trials[i_neuron][subMissTrials], axis=0)\n",
    "\n",
    "        testHit_delta = np.nanmean(testHit_TargetNeuronAverages, axis=0) - np.nanmean(np.nanmean(testHit_TargetNeuronAverages, axis=0)[:session.pre_frames])\n",
    "        testMiss_delta = np.nanmean(testMiss_TargetNeuronAverages, axis=0)- np.nanmean(np.nanmean(testMiss_TargetNeuronAverages, axis=0)[:session.pre_frames])\n",
    "\n",
    "        t = np.arange(0, (session.pre_frames+session.post_frames) - (session.pre_frames+250/30))\n",
    "        testHit_popt, _ = curve_fit(fit_func, t, \n",
    "                                    testHit_delta[int(session.pre_frames+250/30):], \n",
    "                                    p0=(1, 100),\n",
    "                                    maxfev = 10000)\n",
    "        testMiss_popt, _ = curve_fit(fit_func, t, \n",
    "                                     testMiss_delta[int(session.pre_frames+250/30):], \n",
    "                                     p0=(1, 100),\n",
    "                                     maxfev = 10000)\n",
    "        \n",
    "        hit_tau_hats[i_session,i_perm] = testHit_popt[1]\n",
    "        miss_tau_hats[i_session,i_perm] = testMiss_popt[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0166624f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/PulseBasedRecurrencey_hit_tau_hats_1000MaxPerms.npy', hit_tau_hats)\n",
    "np.save('../data/PulseBasedRecurrencey_miss_tau_hats_1000MaxPerms.npy', miss_tau_hats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
